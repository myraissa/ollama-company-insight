{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Brochure Generator Notebook\n",
        "This Jupyter Notebook demonstrates how to scrape a website, extract relevant links, and generate a company brochure using a local Ollama model. Each cell is designed to handle a specific part of the process, from setting up dependencies to generating and displaying the final brochure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1 — Imports & Configuration\n",
        "This cell imports the necessary Python libraries for web scraping, HTTP requests, JSON handling, and Jupyter Notebook display. It also defines configuration variables, such as the URL for the Ollama API and the HTTP headers for web requests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports & Configuration\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from IPython.display import Markdown, display, update_display\n",
        "\n",
        "# Configuration\n",
        "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
        "MODEL = \"llama3.2\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "                  \"(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2 — Ollama Helper Function\n",
        "This cell defines a helper function `ollama_generate` to interact with the local Ollama model. It sends prompts to the model and handles both streaming and non-streaming responses, ensuring robust communication with the API. The function returns the generated text or an empty string if an error occurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ollama Helper Function\n",
        "def ollama_generate(prompt, model=MODEL, stream=False):\n",
        "    \"\"\"Generate text using a local Ollama model.\"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            OLLAMA_URL,\n",
        "            json={\"model\": model, \"prompt\": prompt, \"stream\": stream},\n",
        "            stream=stream\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "\n",
        "        text = \"\"\n",
        "        if stream:\n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    data = json.loads(line)\n",
        "                    if \"response\" in data:\n",
        "                        print(data[\"response\"], end=\"\", flush=True)\n",
        "                        text += data[\"response\"]\n",
        "            print()\n",
        "        else:\n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    data = json.loads(line)\n",
        "                    if \"response\" in data:\n",
        "                        text += data[\"response\"]\n",
        "\n",
        "        return text.strip()\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"⚠️ Error communicating with Ollama server: {e}\")\n",
        "        return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3 — Website Scraper Class\n",
        "This cell defines the `Website` class, which handles fetching and parsing web pages. It uses `requests` to retrieve the page content and `BeautifulSoup` to extract the title, text, and links. The class removes irrelevant elements like scripts and images to focus on textual content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Website Scraper Class\n",
        "class Website:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.body = b\"\"\n",
        "        self.title = \"\"\n",
        "        self.text = \"\"\n",
        "        self.links = []\n",
        "\n",
        "        self._fetch_page()\n",
        "        self._parse_page()\n",
        "\n",
        "    def _fetch_page(self):\n",
        "        try:\n",
        "            response = requests.get(self.url, headers=HEADERS)\n",
        "            print(\"HTTP Status Code:\", response.status_code)\n",
        "            if response.status_code == 200:\n",
        "                self.body = response.content\n",
        "            else:\n",
        "                print(f\"Failed to fetch {self.url}\")\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"⚠️ Error fetching {self.url}: {e}\")\n",
        "\n",
        "    def _parse_page(self):\n",
        "        soup = BeautifulSoup(self.body, 'html.parser')\n",
        "        self.title = soup.title.string if soup.title else \"No title found\"\n",
        "\n",
        "        if soup.body:\n",
        "            for tag in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
        "                tag.decompose()\n",
        "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "        self.links = [urljoin(self.url, a.get(\"href\")) for a in soup.find_all('a') if a.get(\"href\")]\n",
        "\n",
        "    def get_contents(self):\n",
        "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4 — Prompt Definitions\n",
        "This cell defines the system prompts and a helper function for generating user prompts. The `link_system_prompt` ensures the Ollama model returns JSON-formatted links relevant for a brochure, while the `brochure_system_prompt` sets up the model to generate a markdown brochure based on company information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prompt Definitions\n",
        "link_system_prompt = \"\"\"You are provided with a list of links found on a webpage.\n",
        "Return strictly valid JSON only, with no additional text.\n",
        "The JSON format:\n",
        "{\n",
        "    \"links\": [\n",
        "        {\"type\": \"about page\", \"url\": \"https://example.com/about-us/\"}\n",
        "    ]\n",
        "}\n",
        "If no relevant links, return {\"links\": []}.\n",
        "\"\"\"\n",
        "\n",
        "def get_links_user_prompt(website):\n",
        "    prompt = f\"Here is the list of links on {website.url}.\\n\"\n",
        "    prompt += \"Select relevant links for a company brochure. Exclude Terms, Privacy, or emails.\\n\"\n",
        "    prompt += \"\\n\".join(website.links)\n",
        "    return prompt\n",
        "\n",
        "def brochure_system_prompt():\n",
        "    return \"\"\"You are an assistant that creates a short brochure from relevant pages\n",
        "of a company website. Output in markdown including culture, customers, careers/jobs.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5 — Link Extraction Function\n",
        "This cell defines the `get_links` function, which uses the `Website` class to scrape a webpage and extract relevant links for a brochure. It sends the links to the Ollama model with a prompt to filter and format them as JSON, handling any JSON parsing errors gracefully."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Link Extraction Function\n",
        "def get_links(url):\n",
        "    website = Website(url)\n",
        "    prompt = f\"{link_system_prompt}\\n\\n{get_links_user_prompt(website)}\"\n",
        "    response_text = ollama_generate(prompt)\n",
        "\n",
        "    try:\n",
        "        result = json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ JSON parsing error\")\n",
        "        result = {\"links\": []}\n",
        "\n",
        "    if not result.get(\"links\"):\n",
        "        print(\"⚠️ No relevant links found\")\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6 — Brochure Generation Functions\n",
        "This cell contains functions to gather content from a landing page and its relevant links (`get_all_details`), create a user prompt for the brochure (`get_brochure_user_prompt`), and generate the brochure in two modes: normal (`create_brochure`) and streaming (`stream_brochure`). The streaming mode updates the display in real-time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Brochure Generation Functions\n",
        "def get_all_details(url):\n",
        "    result = \"Landing page:\\n\"\n",
        "    landing_page = Website(url)\n",
        "    result += landing_page.get_contents()\n",
        "\n",
        "    links_data = get_links(url)\n",
        "    for link_info in links_data.get(\"links\", []):\n",
        "        page_url = link_info.get(\"url\")\n",
        "        page_type = link_info.get(\"type\", \"Page\")\n",
        "        if not page_url:\n",
        "            continue\n",
        "        page_content = Website(page_url)\n",
        "        result += f\"\\n\\n{page_type}\\n\"\n",
        "        result += page_content.get_contents()\n",
        "    return result\n",
        "\n",
        "def get_brochure_user_prompt(company_name, url):\n",
        "    user_prompt = f\"Company: {company_name}\\n\"\n",
        "    user_prompt += get_all_details(url)\n",
        "    return user_prompt[:5000]  # truncate to avoid overwhelming the model\n",
        "\n",
        "def create_brochure(company_name, url):\n",
        "    prompt = f\"{brochure_system_prompt()}\\n\\n{get_brochure_user_prompt(company_name, url)}\"\n",
        "    result = ollama_generate(prompt)\n",
        "    display(Markdown(result))\n",
        "\n",
        "def stream_brochure(company_name, url):\n",
        "    prompt = f\"{brochure_system_prompt()}\\n\\n{get_brochure_user_prompt(company_name, url)}\"\n",
        "    try:\n",
        "        response = requests.post(OLLAMA_URL, json={\"model\": MODEL, \"prompt\": prompt, \"stream\": True}, stream=True)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        display_handle = display(Markdown(\"\"), display_id=True)\n",
        "        current_text = \"\"\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                data = json.loads(line)\n",
        "                if \"response\" in data:\n",
        "                    current_text += data[\"response\"]\n",
        "                    update_display(Markdown(current_text), display_id=display_handle.display_id)\n",
        "        print(\"\\n✅ Brochure generation complete.\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"⚠️ Error streaming brochure: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7 — Testing the Brochure Generation\n",
        "This cell tests the brochure generation by creating a brochure for Hugging Face using the `create_brochure` function. A commented-out call to `stream_brochure` is included for optional streaming output. Run this cell to see the generated markdown brochure displayed in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Testing Brochure Generation\n",
        "# Normal brochure generation\n",
        "create_brochure(\"Hugging Face\", \"https://huggingface.co\")\n",
        "\n",
        "# Uncomment the following line to test streaming brochure generation\n",
        "# stream_brochure(\"Hugging Face\", \"https://huggingface.co\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}